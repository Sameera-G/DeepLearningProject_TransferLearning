{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Name: Sameera Madushanka Gamage**\n",
        "#**\"Student Number: 2207590\"**\n",
        "#**Model 4**\n",
        "\n",
        "This notebook was prepared in the following way;\n",
        "\n",
        "1.   miniImageNet dataset is downloaded and used for pretraining using \"resnet50\"\n",
        "2.   final layer was edited according to the number of classes\n",
        "3. then the model saved and again loaded for fine tuning using EuroSAT dataset\n",
        "\n",
        "4. then the model final layer was edited again according to the number of classes in EuroSAT dataset\n",
        "\n",
        "5. Randomly selected 100 images representing 5 random classes in the dataset\n",
        "\n",
        "6. then those 100 images were again divided into train_dataset and test_dataset as 25 images for training and 75 for testing (each dataset has 5 classes)\n",
        "\n",
        "7. then using multiple episodes, the EuroSAT dataset's selected 5 classes was trained for fine tuning.\n",
        "\n",
        "8. at each episode the average training andn testing accuracies were accumilated and plotted against the episode number.\n",
        "\n",
        "9. then from among those episodes, the best performing model was automatically saved.\n",
        "\n",
        "10. that best performing model was then loaded and can be used for testing. Testing accuracy of that model has been recorded.\n",
        "\n",
        "11. the model's intention is to get the averge accuracy of all the episodes\n"
      ],
      "metadata": {
        "id": "7B8BnKy9MmC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cleaning Data Folder"
      ],
      "metadata": {
        "id": "3BZq0SyFNDld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2apqe7BfJr8d"
      },
      "outputs": [],
      "source": [
        "!rm -rf *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "sntVIpu6NKOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IiSEvjaSMuQ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "!pip install gdown\n",
        "import gdown\n",
        "import tarfile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the device and selecting"
      ],
      "metadata": {
        "id": "K5CpxGFNNNPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hApJnwSL_Zf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading miniImageNet Dataset"
      ],
      "metadata": {
        "id": "EvCLASe1NOrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_NCE-ThMB5j"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "\n",
        "link1 = 'https://drive.google.com/uc?id=107FTosYIeBn5QbynR46YG91nHcJ70whs'\n",
        "\n",
        "if not os.path.exists('./data_project/miniImageNet'):\n",
        "    print('Downloading dataset')\n",
        "    gdown.download(link1, output='./train.tar')\n",
        "\n",
        "    # Extract the tar file\n",
        "    with tarfile.open('./train.tar', 'r') as tar:\n",
        "        tar.extractall('./data_project/miniImageNet')\n",
        "\n",
        "miniImageNet_path='./data_project/miniImageNet'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Displaying some sample files"
      ],
      "metadata": {
        "id": "6E1ppTbGNSOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t9Js5o4W1p5"
      },
      "outputs": [],
      "source": [
        "# Display some sample images from the training dataset\n",
        "transform_common = transforms.Compose([\n",
        "        transforms.Resize((84, 84)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "miniimagenet_dataset_show = datasets.ImageFolder(root=os.path.join(miniImageNet_path, 'train'), transform=transform_common)\n",
        "sample_loader = torch.utils.data.DataLoader(miniimagenet_dataset_show, batch_size=4, shuffle=True)\n",
        "sample_batch, sample_labels = next(iter(sample_loader))\n",
        "\n",
        "# Display sample images\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Sample Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(sample_batch, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgNupibzofwz"
      },
      "source": [
        "#Loading data, Transform defining, Data Splitting, DataLoader definining and Activity showcasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdQ5sFQgMEZs"
      },
      "outputs": [],
      "source": [
        "# Load miniImageNet dataset\n",
        "\n",
        "# Training transformations with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=216, scale=(0.7, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "#downloadingn different instances of data for different transforms\n",
        "miniimagenet_dataset_training = datasets.ImageFolder(root=os.path.join(miniImageNet_path, 'train'), transform=train_transform)\n",
        "miniimagenet_dataset_val = datasets.ImageFolder(root=os.path.join(miniImageNet_path, 'train'), transform=val_transform)\n",
        "miniimagenet_dataset_testing = datasets.ImageFolder(root=os.path.join(miniImageNet_path, 'train'), transform=test_transform)\n",
        "\n",
        "# Split the miniImageNet dataset into train and validation sets\n",
        "train_size = int(0.6 * len(miniimagenet_dataset_training))\n",
        "val_size = int(0.2 * len(miniimagenet_dataset_training))\n",
        "test_size = len(miniimagenet_dataset_training) - train_size - val_size\n",
        "\n",
        "#for transform of train\n",
        "train_dataset, _, _ = torch.utils.data.random_split(miniimagenet_dataset_training,\n",
        "                                                                         [train_size, val_size, test_size])\n",
        "#for transform of validation\n",
        "_, val_dataset, _ = torch.utils.data.random_split(miniimagenet_dataset_val,\n",
        "                                                                         [train_size, val_size, test_size])\n",
        "#for transform of testing\n",
        "_, _, test_dataset = torch.utils.data.random_split(miniimagenet_dataset_testing,\n",
        "                                                                         [train_size, val_size, test_size])\n",
        "\n",
        "# Apply the transforms to the datasets\n",
        "train_dataset.dataset.transform=train_transform\n",
        "val_dataset.dataset.transform=val_transform\n",
        "test_dataset.dataset.transform=test_transform\n",
        "\n",
        "# Create data loaders for miniImageNet\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "#Size Data\n",
        "num_classes_miniimagenet = len(miniimagenet_dataset_training.classes)\n",
        "print(\"Number of Classes: \",num_classes_miniimagenet)\n",
        "print(\"Train Size: \", train_size)\n",
        "print(\"Val Size: \", val_size)\n",
        "print(\"Test Size: \", test_size)\n",
        "\n",
        "print(\"Train Dataset Transform: \", train_dataset.dataset.transform)\n",
        "print(\"Validation Dataset Transform: \", val_dataset.dataset.transform)\n",
        "print(\"Test Dataset Transform: \", test_dataset.dataset.transform)\n",
        "\n",
        "class_indices = miniimagenet_dataset_training.class_to_idx\n",
        "print(class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the data type of the images after transforming"
      ],
      "metadata": {
        "id": "S5LtP-I2NeAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ytvxUyjr8fL"
      },
      "outputs": [],
      "source": [
        "# Function to check the data type of images in a batch\n",
        "def check_image_data_type(loader):\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    # Check the data type of the images\n",
        "    data_type = type(images)\n",
        "\n",
        "    return data_type\n",
        "\n",
        "# Check the data type for train_loader\n",
        "train_data_type = check_image_data_type(train_loader)\n",
        "print(\"Train Loader Image Data Type:\", train_data_type)\n",
        "\n",
        "# Check the data type for val_loader\n",
        "val_data_type = check_image_data_type(val_loader)\n",
        "print(\"Validation Loader Image Data Type:\", val_data_type)\n",
        "\n",
        "# Check the data type for test_loader\n",
        "test_data_type = check_image_data_type(test_loader)\n",
        "print(\"Test Loader Image Data Type:\", test_data_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Displaying images after applying transfoms"
      ],
      "metadata": {
        "id": "jlTyTKqrNg7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9XlzsalrAzp"
      },
      "outputs": [],
      "source": [
        "# display images in a grid with normalization\n",
        "def show_images(images, title):\n",
        "    num_images = len(images)\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        ax = axes[i] if num_images > 1 else axes\n",
        "\n",
        "        # Normalize the image tensor to [0, 1] range\n",
        "        img = np.transpose(images[i].numpy(), (1, 2, 0))\n",
        "        img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"Sample {i + 1}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# get a sample from the data loader\n",
        "def get_sample(loader):\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "    return images, labels\n",
        "\n",
        "# Get a sample from each dataset\n",
        "train_images, _ = get_sample(train_loader)\n",
        "val_images, _ = get_sample(val_loader)\n",
        "test_images, _ = get_sample(test_loader)\n",
        "\n",
        "# Show the original and transformed images for each dataset\n",
        "show_images([train_images[0], train_images[1]], \"Training Dataset\")\n",
        "show_images([val_images[0], val_images[1]], \"Validation Dataset\")\n",
        "show_images([test_images[0], test_images[1]], \"Test Dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading the model resnet50 and its weights"
      ],
      "metadata": {
        "id": "awIzRU0-NmOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmzwnuFHFOMr"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "# selecting deep learning architecture\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "weights=\"IMAGENET1K_V2\"\n",
        "pretrained_model = resnet50(weights=weights)\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "#print(pretrained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the model summary\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6MAY5iIXNoL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFfM_3YKPCIz"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "#summary(pretrained_model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modifying final layer to align with the dataset classes"
      ],
      "metadata": {
        "id": "mXQvuEZKNpie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55TZJPvUSVkd"
      },
      "outputs": [],
      "source": [
        "# unfreeze all layers\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = True\n",
        "    param.data = param.data.to(device)\n",
        "\n",
        "# Modify the final classification layer\n",
        "num_classes_miniimagenet = len(miniimagenet_dataset_training.classes)\n",
        "dropout = 0.6\n",
        "pretrained_model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=dropout),\n",
        "    nn.Linear(pretrained_model.fc.in_features, num_classes_miniimagenet)\n",
        ")\n",
        "\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "#pretrained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Function"
      ],
      "metadata": {
        "id": "d5MKoqOiNuUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbdBAatzF2Sz"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "class Trainer:\n",
        "    def __init__(self, pretrained_model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs):\n",
        "      self.pretrained_model = pretrained_model\n",
        "      self.train_loader = train_loader\n",
        "      self.val_loader = val_loader\n",
        "      self.optimizer = optimizer\n",
        "      self.criterion = criterion\n",
        "      self.scheduler = scheduler\n",
        "      self.num_epochs = num_epochs\n",
        "    def training(self):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(device)\n",
        "        self.pretrained_model = self.pretrained_model.to(device)\n",
        "\n",
        "        history = {'train_loss': [], 'valid_loss': [], 'train_acc': [], 'valid_acc': []}\n",
        "        best_valid_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.pretrained_model.train()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            true_labels_train = []\n",
        "            predicted_labels_train = []\n",
        "\n",
        "            # Loss and Accuracy within the epoch\n",
        "            train_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            valid_acc = 0.0\n",
        "\n",
        "            for batch_idx, (inputs, labels) in enumerate(self.train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.pretrained_model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Compute the accuracy\n",
        "                predictions = torch.max(outputs.data, 1)[1]\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "                train_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                true_labels_train.extend(labels.cpu().numpy())\n",
        "                predicted_labels_train.extend(predictions.cpu().numpy())\n",
        "\n",
        "            average_loss = total_loss / len(self.train_loader)\n",
        "            train_accuracy = accuracy_score(true_labels_train, predicted_labels_train)\n",
        "            history['train_loss'].append(average_loss)\n",
        "            history['train_acc'].append(train_accuracy)\n",
        "\n",
        "            # Validation\n",
        "            true_labels_val = []\n",
        "            predicted_labels_val = []\n",
        "\n",
        "            self.pretrained_model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in self.val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = self.pretrained_model(inputs)\n",
        "\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    predictions = torch.max(outputs.data, 1)[1]\n",
        "                    correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "                    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "                    valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                    true_labels_val.extend(labels.cpu().numpy())\n",
        "                    predicted_labels_val.extend(predictions.cpu().numpy())\n",
        "\n",
        "                # Map class indices to class names\n",
        "                class_idx_to_name = {idx: class_name for class_name, idx in class_indices.items()}\n",
        "                true_labels_val = [class_idx_to_name[idx] for idx in true_labels_val]\n",
        "                predicted_labels_val = [class_idx_to_name[idx] for idx in predicted_labels_val]\n",
        "\n",
        "                # Calculate the average validation loss and validation accuracy\n",
        "                avg_valid_loss = valid_loss / len(self.val_loader.dataset)\n",
        "                avg_valid_acc = accuracy_score(true_labels_val, predicted_labels_val)\n",
        "\n",
        "                history['valid_loss'].append(avg_valid_loss)\n",
        "                history['valid_acc'].append(avg_valid_acc)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Training Loss: {average_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {avg_valid_acc:.4f}\")\n",
        "\n",
        "            # Update the learning rate after each epoch\n",
        "            scheduler.step()\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer"
      ],
      "metadata": {
        "id": "IdztP-raN3vr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf_V1BK5rQYP"
      },
      "outputs": [],
      "source": [
        "#Optimizer\n",
        "optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=0.00001, weight_decay=0.001)\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Process"
      ],
      "metadata": {
        "id": "3riyg9X7N5BP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQv8-43uhItT"
      },
      "outputs": [],
      "source": [
        "#Training process\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainer = Trainer(pretrained_model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=15)\n",
        "history = trainer.training()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting loss and accuracy variations"
      ],
      "metadata": {
        "id": "5XxhSVtHN8n_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90X4rtArcZYD"
      },
      "outputs": [],
      "source": [
        "# Plotting loss and accuracy variations\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Training Loss')\n",
        "plt.plot(history['valid_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "plt.plot(history['valid_acc'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "bwd0WSOsN-XT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_P8fO-xymqK"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "def testSetAccuracy(model, test_loader, criterion):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    true_labels_test = []\n",
        "    predicted_labels_test = []\n",
        "\n",
        "    # Store test accuracy history\n",
        "    test_acc_history = []\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Test loop\n",
        "        for j, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate the loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            #accuracy calculation\n",
        "            true_labels_test.extend(labels.cpu().numpy())\n",
        "            predicted_labels_test.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct_counts = torch.sum(torch.argmax(outputs, 1) == labels)\n",
        "            acc = correct_counts.item() / labels.size(0)\n",
        "\n",
        "            # collect accuracy\n",
        "            test_acc += acc * inputs.size(0)\n",
        "\n",
        "            # collect test accuracy at each batch\n",
        "            test_acc_history.append(acc)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc))\n",
        "\n",
        "    # Calculate average test loss and test accuracy\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    avg_test_acc = test_acc / len(test_loader.dataset)\n",
        "\n",
        "    # Calculate accuracy using scikit-learn\n",
        "    test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
        "\n",
        "    print(\"Test Accuracy (from sklearn): {:.4f}\".format(test_accuracy))\n",
        "    print(\"Average Test Loss: {:.4f}, Average Test Accuracy: {:.4f}\".format(avg_test_loss, avg_test_acc))\n",
        "\n",
        "    # Return test accuracy history\n",
        "    return avg_test_loss, avg_test_acc, test_accuracy, test_acc_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Process"
      ],
      "metadata": {
        "id": "IHREhwM5OFqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_gHFanAkwap"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "avg_test_loss, avg_test_acc, test_accuracy, test_acc_history = testSetAccuracy(pretrained_model, test_loader, criterion)\n",
        "print(\"avg_test_loss: \", avg_test_loss)\n",
        "print(\"avg_test_Accuracy: \", avg_test_acc)\n",
        "print(\"test_Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test accuracy variation"
      ],
      "metadata": {
        "id": "CGPFWRFoONg-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seZkAid5EcSY"
      },
      "outputs": [],
      "source": [
        "# Plot test accuracy variation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(test_acc_history, label='Test Accuracy')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy Variation Over Batches')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD5DDB4TFzZx"
      },
      "source": [
        "#Saving the pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFCrciSCFy5y"
      },
      "outputs": [],
      "source": [
        "# Save the pretrained model\n",
        "torch.save(pretrained_model, 'pretrained_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwI28Tzx_GSR"
      },
      "source": [
        "#Download EuroSAT data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCCC9lXx_FwF"
      },
      "outputs": [],
      "source": [
        "link2 = 'https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1'\n",
        "if not os.path.exists('./data_project/EuroSAT_RGB'):\n",
        "    print('Downloading dataset')\n",
        "    gdown.download(link2, output='./EuroSAT_RGB.zip')\n",
        "\n",
        "    # Unzip\n",
        "    with zipfile.ZipFile('./EuroSAT_RGB.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('./data_project/EuroSAT_RGB')\n",
        "\n",
        "EuroSAT_RGB_path='./data_project/EuroSAT_RGB'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Displaying sample images"
      ],
      "metadata": {
        "id": "EAXm1rYOOVgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHchuMbj_f8i"
      },
      "outputs": [],
      "source": [
        "# Display some sample images from the training dataset\n",
        "transform_common = transforms.Compose([\n",
        "        transforms.Resize((84, 84)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "eurosat_dataset_show = datasets.ImageFolder(root=os.path.join(EuroSAT_RGB_path, 'EuroSAT_RGB'), transform=transform_common)\n",
        "sample_loader = torch.utils.data.DataLoader(eurosat_dataset_show, batch_size=4, shuffle=True)\n",
        "sample_batch, sample_labels = next(iter(sample_loader))\n",
        "\n",
        "# Display sample images\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Sample Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(sample_batch, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select 100 images and 5 different classes randomly"
      ],
      "metadata": {
        "id": "nK-IlpJbOY_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZAhvk_6NKLp"
      },
      "outputs": [],
      "source": [
        "#Select 100 images and 5 different classes randomly\n",
        "\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "# Path to the root directory containing the 10 folders\n",
        "root_path = \"./data_project/EuroSAT_RGB/EuroSAT_RGB\"\n",
        "\n",
        "# Output path for the new dataset\n",
        "output_path = \"./data_project/EuroSAT_selected_images\"\n",
        "if output_path == './data_project/EuroSAT_selected_images':\n",
        "  !rm -rf EuroSAT_selected_images\n",
        "\n",
        "# Get a list of all folders in the root directory\n",
        "all_folders = os.listdir(root_path)\n",
        "\n",
        "# Randomly select 5 folders\n",
        "selected_folders = random.sample(all_folders, 5)\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Iterate through selected folders\n",
        "for folder in selected_folders:\n",
        "  folder_path = os.path.join(root_path, folder)\n",
        "\n",
        "  # Get a list of all files in the folder\n",
        "  all_files = os.listdir(folder_path)\n",
        "\n",
        "  # Randomly select 20 images\n",
        "  selected_images = random.sample(all_files, 20)\n",
        "\n",
        "  # Create a subdirectory in the output path for the current folder\n",
        "  output_folder_path = os.path.join(output_path, folder)\n",
        "  os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "  # Copy selected images to the output folder\n",
        "  for image in selected_images:\n",
        "      src_path = os.path.join(folder_path, image)\n",
        "      dest_path = os.path.join(output_folder_path, image)\n",
        "      copyfile(src_path, dest_path)\n",
        "\n",
        "print(\"Dataset creation complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#separate image folders into train and test"
      ],
      "metadata": {
        "id": "GHaE8wnsOaT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlaRLLd3p_Yh"
      },
      "outputs": [],
      "source": [
        "#separate image folders into train val test\n",
        "\n",
        "import random\n",
        "import os\n",
        "from shutil import copyfile\n",
        "def train_test_eurosat():\n",
        "# Path to the root directory containing the folders\n",
        "  root_path = \"./data_project/EuroSAT_selected_images\"\n",
        "\n",
        "  # Output path for the new dataset\n",
        "  output_path = \"./data_project/EuroSAT_tuning\"\n",
        "  if output_path == \"./data_project/EuroSAT_tuning\":\n",
        "      !rm -rf EuroSAT_selected_images\n",
        "\n",
        "  # Get a list of all folders in the root directory\n",
        "  all_folders = os.listdir(root_path)\n",
        "\n",
        "  # Create the output directory if it doesn't exist\n",
        "  os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "  # Create subdirectories for train, val, and test datasets\n",
        "  train_path = os.path.join(output_path, \"train_dataset\")\n",
        "  test_path = os.path.join(output_path, \"test_dataset\")\n",
        "  os.makedirs(train_path, exist_ok=True)\n",
        "  os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "  # Iterate through selected folders\n",
        "  for folder in all_folders:\n",
        "    folder_path = os.path.join(root_path, folder)\n",
        "    # Create subdirectories for each class within train and test datasets\n",
        "    train_class_path = os.path.join(train_path, folder)\n",
        "    test_class_path = os.path.join(test_path, folder)\n",
        "\n",
        "    os.makedirs(train_class_path, exist_ok=True)\n",
        "    os.makedirs(test_class_path, exist_ok=True)\n",
        "\n",
        "    # Get a list of all files in the folder\n",
        "    all_files = os.listdir(folder_path)\n",
        "\n",
        "    # Randomly select 5 images for training\n",
        "    train_images = random.sample(all_files, 5)\n",
        "    for img in train_images:\n",
        "        src_path = os.path.join(folder_path, img)\n",
        "        dest_path = os.path.join(train_class_path, img)\n",
        "        copyfile(src_path, dest_path)\n",
        "\n",
        "    # Put remaining images into the test dataset\n",
        "    test_images = set(all_files) - set(train_images)\n",
        "    for img in test_images:\n",
        "        src_path = os.path.join(folder_path, img)\n",
        "        dest_path = os.path.join(test_class_path, img)\n",
        "        copyfile(src_path, dest_path)\n",
        "\n",
        "  print(\"Dataset creation complete.\")\n",
        "\n",
        "#using\n",
        "train_test_eurosat()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating transforms and data loaders"
      ],
      "metadata": {
        "id": "1gW8LuLOOqcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXCqvM9QBCyz"
      },
      "outputs": [],
      "source": [
        "def load_eurosat_dataset(data_path='./data_project/EuroSAT_tuning/', batch_size=25):\n",
        "    # Transforms\n",
        "    image_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(size=216, scale=(0.7, 1.0)),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomGrayscale(p=0.1),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'test': transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    # Set train and test directory paths\n",
        "    train_dataset_path = os.path.join(data_path, 'train_dataset')\n",
        "    test_dataset_path = os.path.join(data_path, 'test_dataset')\n",
        "\n",
        "    # Number of classes\n",
        "    num_classes_eurosat = len(os.listdir(train_dataset_path))\n",
        "\n",
        "    # Load Data from folders\n",
        "    data = {\n",
        "        'train': datasets.ImageFolder(root=train_dataset_path, transform=image_transforms['train']),\n",
        "        'test': datasets.ImageFolder(root=test_dataset_path, transform=image_transforms['test'])\n",
        "    }\n",
        "\n",
        "    # mapping of the indices to the class names\n",
        "    idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "\n",
        "    # Size of Data\n",
        "    train_data_size = len(data['train'])\n",
        "    test_data_size = len(data['test'])\n",
        "\n",
        "    # Create iterators for the Data loaded using DataLoader module\n",
        "    train_loader = torch.utils.data.DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(data['test'], batch_size=batch_size, shuffle=False)\n",
        "    print(\"Train Data Size:\", train_data_size)\n",
        "    print(\"Test Data Size:\", test_data_size)\n",
        "\n",
        "    return train_loader, test_loader, idx_to_class, num_classes_eurosat\n",
        "\n",
        "# Running\n",
        "train_loader, test_loader, idx_to_class, num_classes = load_eurosat_dataset()\n",
        "class_indices = idx_to_class\n",
        "print(\"Class Indices:\", idx_to_class)\n",
        "print(\"Number of Classes:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# display images with class names"
      ],
      "metadata": {
        "id": "1IlDAJiROwLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxYRyaj2a01n"
      },
      "outputs": [],
      "source": [
        "# Function to display images with class names\n",
        "def show_images(images, labels, class_names, title):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.suptitle(title, y=1.02, fontsize=16)\n",
        "\n",
        "    for i in range(min(5, len(images))):\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(np.transpose(images[i].numpy(), (1, 2, 0)))\n",
        "        plt.title(class_names[labels[i].item()])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# get sample images with class labels from a DataLoader\n",
        "def get_sample(loader, class_names):\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "    return images, labels, class_names\n",
        "\n",
        "# Get a sample from each dataset\n",
        "class_names_euroSAT = idx_to_class\n",
        "train_images, train_labels, _ = get_sample(train_loader, class_names_euroSAT)\n",
        "test_images, test_labels, _ = get_sample(test_loader, class_names_euroSAT)\n",
        "\n",
        "Train = 'Train'\n",
        "Test = 'Test'\n",
        "# Show sample images with class names\n",
        "def show_sample_images(title, images, labels):\n",
        "  plt.figure(figsize=(10, 3))\n",
        "  plt.suptitle(title + \"Dataset Samples\", y=1.02, fontsize=16)\n",
        "  for i in range(min(5, len(images))):\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        image_np = np.transpose(images[i].numpy(), (1, 2, 0))\n",
        "        image_np = np.clip(image_np, 0, 1)  # Clip pixel values to the valid range\n",
        "        plt.imshow(image_np)\n",
        "        plt.title(class_names_euroSAT[labels[i].item()])\n",
        "        plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "show_sample_images(Train, train_images, train_labels)\n",
        "show_sample_images(Test, test_images, test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking Data type"
      ],
      "metadata": {
        "id": "C2GVo3x9O0On"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfI8AHU3CuK9"
      },
      "outputs": [],
      "source": [
        "# check the data type of images in a batch\n",
        "def check_image_data_type(loader):\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    # Check the data type of the images\n",
        "    data_type = type(images)\n",
        "\n",
        "    return data_type\n",
        "\n",
        "# Check the data type for train_loader\n",
        "train_data_type = check_image_data_type(train_loader)\n",
        "print(\"Train Loader Image Data Type:\", train_data_type)\n",
        "\n",
        "# Check the data type for test_loader\n",
        "test_data_type = check_image_data_type(test_loader)\n",
        "print(\"Test Loader Image Data Type:\", test_data_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY_CaW63Gra-"
      },
      "source": [
        "#Loading the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3GD9gs3Gq1W"
      },
      "outputs": [],
      "source": [
        "#Loading the model\n",
        "\n",
        "modified_trained_model = torch.load('pretrained_model.pth')\n",
        "modified_trained_model = modified_trained_model.to(device)\n",
        "\n",
        "# Setting to evaluation mode\n",
        "modified_trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iJumiZO8jpT"
      },
      "source": [
        "#Adjusting final layer to match class size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZnqAFk2fkZC"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers\n",
        "for param in modified_trained_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    param.data = param.data.to(device)\n",
        "\n",
        "# Modify the final classification layer\n",
        "num_classes_eurosat = len(class_indices)\n",
        "dropout = 0.7\n",
        "\n",
        "# Get the number of input features from the last layer in the Sequential module\n",
        "in_features = modified_trained_model.fc[-1].in_features\n",
        "\n",
        "# Replace the final fully connected layer with a new layer including dropout\n",
        "modified_trained_model.fc = nn.Sequential(\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=dropout),\n",
        "    nn.Linear(in_features, num_classes_eurosat)\n",
        ")\n",
        "\n",
        "# Unfreeze the final fully connected layer\n",
        "modified_trained_model.fc.requires_grad = True\n",
        "\n",
        "# Move the model to the device\n",
        "modified_trained_model = modified_trained_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "5p8x9sr0O8NW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd-rEPb1W8YO"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, optimizer, criterion, scheduler, num_epochs):\n",
        "      self.model = model\n",
        "      self.train_loader = train_loader\n",
        "      self.optimizer = optimizer\n",
        "      self.criterion = criterion\n",
        "      self.scheduler = scheduler\n",
        "      self.num_epochs = num_epochs\n",
        "\n",
        "    def training(self):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(device)\n",
        "        self.model = self.model.to(device)\n",
        "\n",
        "        history = {'train_loss': [], 'train_acc': []}\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            true_labels_train = []\n",
        "            predicted_labels_train = []\n",
        "\n",
        "            train_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "\n",
        "            for batch_idx, (inputs, labels) in enumerate(self.train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Compute the accuracy\n",
        "                predictions = torch.max(outputs.data, 1)[1]\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "                train_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                true_labels_train.extend(labels.cpu().numpy())\n",
        "                predicted_labels_train.extend(predictions.cpu().numpy())\n",
        "\n",
        "            average_loss = total_loss / len(self.train_loader)\n",
        "            train_accuracy = accuracy_score(true_labels_train, predicted_labels_train)\n",
        "            history['train_loss'].append(average_loss)\n",
        "            history['train_acc'].append(train_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Training Loss: {average_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "            # Update learning rate after each epoch\n",
        "            scheduler.step()\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n2ASCycMlHF"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "def testSetAccuracy(model, test_loader, criterion):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    true_labels_test = []\n",
        "    predicted_labels_test = []\n",
        "\n",
        "    # Store test accuracy history\n",
        "    test_acc_history = []\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Test loop\n",
        "        for j, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate the loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Convert predictions to numpy arrays for accuracy calculation\n",
        "            true_labels_test.extend(labels.cpu().numpy())\n",
        "            predicted_labels_test.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct_counts = torch.sum(torch.argmax(outputs, 1) == labels)\n",
        "            acc = correct_counts.item() / labels.size(0)\n",
        "\n",
        "            # Accumulate accuracy\n",
        "            test_acc += acc * inputs.size(0)\n",
        "\n",
        "            # collect test accuracy at each batch\n",
        "            test_acc_history.append(acc)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc))\n",
        "\n",
        "    # Calculate average test loss and test accuracy\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    avg_test_acc = test_acc / len(test_loader.dataset)\n",
        "\n",
        "    # Calculate accuracy using scikit-learn\n",
        "    test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
        "\n",
        "    print(\"Test Accuracy (from sklearn): {:.4f}\".format(test_accuracy))\n",
        "    print(\"Average Test Loss: {:.4f}, Average Test Accuracy: {:.4f}\".format(avg_test_loss, avg_test_acc))\n",
        "\n",
        "    # Return test accuracy history\n",
        "    return avg_test_loss, avg_test_acc, test_accuracy, test_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Episode training and testing - fine tuning"
      ],
      "metadata": {
        "id": "ZKa1DDF7PLMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAe4_z14XC9z"
      },
      "outputs": [],
      "source": [
        "#Episode training and testing\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 30\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 7\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.001\n",
        "\n",
        "# Create an instance of the Trainer\n",
        "optimizer = torch.optim.Adam(modified_trained_model.parameters(), lr=lr, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "episode_history = {'average_train_loss': [], 'average_train_acc': [], 'average_test_loss': [], 'average_test_acc': []}\n",
        "\n",
        "# Run training for multiple episodes\n",
        "for episode in range(num_episodes):\n",
        "    print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Run the training process for a one episode\n",
        "    trainer = Trainer(modified_trained_model, train_loader, optimizer, criterion, scheduler, num_epochs=num_epochs)\n",
        "\n",
        "    #data loaders\n",
        "    train_loader, test_loader, idx_to_class, num_classes = load_eurosat_dataset()\n",
        "\n",
        "    # Get a sample from each dataset to print\n",
        "    class_names_euroSAT = idx_to_class\n",
        "    train_images, train_labels, _ = get_sample(train_loader, class_names_euroSAT)\n",
        "    test_images, test_labels, _ = get_sample(test_loader, class_names_euroSAT)\n",
        "\n",
        "    #show sample images\n",
        "    show_sample_images(Train, train_images, train_labels)\n",
        "    show_sample_images(Test, test_images, test_labels)\n",
        "\n",
        "    # Train the model\n",
        "    history = trainer.training()\n",
        "\n",
        "    # return the average training loss and accuracy\n",
        "    average_training_loss = sum(history['train_loss']) / len(history['train_loss'])\n",
        "    average_training_accuracy = sum(history['train_acc']) / len(history['train_acc'])\n",
        "    episode_history['average_train_loss'].append(average_training_loss)\n",
        "    episode_history['average_train_acc'].append(average_training_accuracy)\n",
        "\n",
        "    #Testing\n",
        "    _, test_loader, _, _  = load_eurosat_dataset()\n",
        "    avg_test_loss, avg_test_acc, test_accuracy, test_acc_history = testSetAccuracy(modified_trained_model, test_loader, criterion)\n",
        "    print(\"avg_test_loss: \", avg_test_loss)\n",
        "    print(\"avg_test_Accuracy: \", avg_test_acc)\n",
        "    print(\"test_Accuracy: {:.4f}\".format(test_accuracy))\n",
        "    episode_history['average_test_loss'].append(avg_test_loss)\n",
        "    episode_history['average_test_acc'].append(avg_test_acc)\n",
        "\n",
        "    # Update the best model if the current accuracy is higher\n",
        "    if average_training_accuracy > best_accuracy:\n",
        "        best_accuracy = average_training_accuracy\n",
        "        best_model = modified_trained_model\n",
        "    !rm -rf EuroSAT_selected_images\n",
        "    !rm -rf EuroSAT_tuning\n",
        "\n",
        "    print(f\"Average Training Loss: {average_training_loss:.4f}, Average Training Accuracy: {average_training_accuracy:.4f}\")\n",
        "\n",
        "    # Update the learning rate after each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model, 'best_fine_tuned_model.pth')\n",
        "print(\"Best model saved.\")\n",
        "print(episode_history)\n",
        "###################################\n",
        "# Calculate average for 'average_train_loss'\n",
        "avg_train_loss = sum(episode_history['average_train_loss']) / len(episode_history['average_train_loss']) if episode_history['average_train_loss'] else 0.0\n",
        "# Calculate average for 'average_train_acc'\n",
        "avg_train_acc = sum(episode_history['average_train_acc']) / len(episode_history['average_train_acc']) if episode_history['average_train_acc'] else 0.0\n",
        "# Calculate average for 'average_test_loss'\n",
        "avg_test_loss = sum(episode_history['average_test_loss']) / len(episode_history['average_test_loss']) if episode_history['average_test_loss'] else 0.0\n",
        "# Calculate average for 'average_test_acc'\n",
        "avg_test_acc = sum(episode_history['average_test_acc']) / len(episode_history['average_test_acc']) if episode_history['average_test_acc'] else 0.0\n",
        "print(f'Average Train Loss: {avg_train_loss}')\n",
        "print(f'Average Train Accuracy: {avg_train_acc}')\n",
        "print(f'Average Test Loss: {avg_test_loss}')\n",
        "print(f'Average Test Accuracy: {avg_test_acc}')\n",
        "####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting loss and accuracy variations"
      ],
      "metadata": {
        "id": "e18K6rXfPal5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlepqglnXZ5N"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "# Calculate average for 'average_train_loss'\n",
        "avg_train_loss = sum(episode_history['average_train_loss']) / len(episode_history['average_train_loss']) if episode_history['average_train_loss'] else 0.0\n",
        "# Calculate average for 'average_train_acc'\n",
        "avg_train_acc = sum(episode_history['average_train_acc']) / len(episode_history['average_train_acc']) if episode_history['average_train_acc'] else 0.0\n",
        "# Calculate average for 'average_test_loss'\n",
        "avg_test_loss = sum(episode_history['average_test_loss']) / len(episode_history['average_test_loss']) if episode_history['average_test_loss'] else 0.0\n",
        "# Calculate average for 'average_test_acc'\n",
        "avg_test_acc = sum(episode_history['average_test_acc']) / len(episode_history['average_test_acc']) if episode_history['average_test_acc'] else 0.0\n",
        "print(f'Average Train Loss: {avg_train_loss}')\n",
        "print(f'Average Train Accuracy: {avg_train_acc}')\n",
        "print(f'Average Test Loss: {avg_test_loss}')\n",
        "print(f'Average Test Accuracy: {avg_test_acc}')\n",
        "####################################\n",
        "# Plotting loss and accuracy variations\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_history['average_train_loss'], label='Average Training Loss')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(episode_history['average_train_acc'], label='Average Training Accuracy')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(episode_history['average_test_loss'], label='Average Test Loss')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(episode_history['average_test_acc'], label='Average Test Accuracy')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWjPnaN0MlHG"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "# Calculate average for 'average_train_loss'\n",
        "avg_train_loss = sum(episode_history['average_train_loss']) / len(episode_history['average_train_loss']) if episode_history['average_train_loss'] else 0.0\n",
        "# Calculate average for 'average_train_acc'\n",
        "avg_train_acc = sum(episode_history['average_train_acc']) / len(episode_history['average_train_acc']) if episode_history['average_train_acc'] else 0.0\n",
        "# Calculate average for 'average_test_loss'\n",
        "avg_test_loss = sum(episode_history['average_test_loss']) / len(episode_history['average_test_loss']) if episode_history['average_test_loss'] else 0.0\n",
        "# Calculate average for 'average_test_acc'\n",
        "avg_test_acc = sum(episode_history['average_test_acc']) / len(episode_history['average_test_acc']) if episode_history['average_test_acc'] else 0.0\n",
        "print(f'Average Train Loss: {avg_train_loss}')\n",
        "print(f'Average Train Accuracy: {avg_train_acc}')\n",
        "print(f'Average Test Loss: {avg_test_loss}')\n",
        "print(f'Average Test Accuracy: {avg_test_acc}')\n",
        "####################################\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting training loss and accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_history['average_train_loss'], label='Training Loss', color='blue')\n",
        "plt.plot(episode_history['average_test_loss'], label='Test Loss', color='orange')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(episode_history['average_train_acc'], label='Training Accuracy', color='blue')\n",
        "plt.plot(episode_history['average_test_acc'], label='Test Accuracy', color='orange')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuxz2TuIMlHH"
      },
      "source": [
        "# Getting the best model from the above and using for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc12fXVGQSFN"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "best_fine_tuned_model = torch.load('best_fine_tuned_model.pth')\n",
        "\n",
        "# Move the model to the device\n",
        "best_fine_tuned_model = best_fine_tuned_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "best_fine_tuned_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "IF6vFk5QPiYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfcywEjEXhwI"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "def testSetAccuracy(model, test_loader, criterion):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "    true_labels_test = []\n",
        "    predicted_labels_test = []\n",
        "\n",
        "    # Store test accuracy history\n",
        "    test_acc_history = []\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Test loop\n",
        "        for j, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate the loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Convert predictions to numpy arrays for accuracy calculation\n",
        "            true_labels_test.extend(labels.cpu().numpy())\n",
        "            predicted_labels_test.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct_counts = torch.sum(torch.argmax(outputs, 1) == labels)\n",
        "            acc = correct_counts.item() / labels.size(0)\n",
        "\n",
        "            # Accumulate accuracy\n",
        "            test_acc += acc * inputs.size(0)\n",
        "\n",
        "            # collect test accuracy at each batch\n",
        "            test_acc_history.append(acc)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc))\n",
        "\n",
        "    # average test loss and test accuracy\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    avg_test_acc = test_acc / len(test_loader.dataset)\n",
        "\n",
        "    # accuracy using scikit-learn\n",
        "    test_accuracy = accuracy_score(true_labels_test, predicted_labels_test)\n",
        "\n",
        "    print(\"Test Accuracy (from sklearn): {:.4f}\".format(test_accuracy))\n",
        "    print(\"Average Test Loss: {:.4f}, Average Test Accuracy: {:.4f}\".format(avg_test_loss, avg_test_acc))\n",
        "\n",
        "    # Return test accuracy history\n",
        "    return avg_test_loss, avg_test_acc, test_accuracy, test_acc_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#testing process"
      ],
      "metadata": {
        "id": "07kIbvPrPmCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfHdaM5zXmnj"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "_, test_loader, _, _  = load_eurosat_dataset()\n",
        "avg_test_loss, avg_test_acc, test_accuracy, test_acc_history = testSetAccuracy(best_fine_tuned_model, test_loader, criterion)\n",
        "print(\"avg_test_loss: \", avg_test_loss)\n",
        "print(\"avg_test_Accuracy: \", avg_test_acc)\n",
        "print(\"test_Accuracy: {:.4f}\".format(test_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plotting the testing accuracy"
      ],
      "metadata": {
        "id": "jBhBFR4gPsd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE_rFXOCXzTr"
      },
      "outputs": [],
      "source": [
        "_, _, _, test_acc_history = testSetAccuracy(best_fine_tuned_model, test_loader, criterion)\n",
        "# Plot test accuracy variation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(test_acc_history, label='Test Accuracy')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy Variation Over Batches')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#checking the model summary"
      ],
      "metadata": {
        "id": "PtSpB-KjPvK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_BLJ_7kwvuo"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(modified_trained_model, (3, 128, 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predictions using web images"
      ],
      "metadata": {
        "id": "qtRs37e7Pywl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu9Tm3djxkgK"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader, class_indices, num_classes = load_eurosat_dataset()\n",
        "\n",
        "image_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'test': transforms.Compose([\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    }\n",
        "\n",
        "def predict(model, test_image_name):\n",
        "    transform = image_transforms['test']\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "\n",
        "    # Apply transformation\n",
        "    test_image = transform(test_image)\n",
        "\n",
        "    plt.imshow(test_image.permute(1, 2, 0))\n",
        "\n",
        "    test_image_tensor = test_image.unsqueeze(0)\n",
        "    test_image_tensor = test_image_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "\n",
        "        topk, topclass = ps.topk(3, dim=1)\n",
        "        print(\"Class Indices:\", idx_to_class)\n",
        "        print(\"Model Output:\", topclass.cpu().numpy()[0])\n",
        "\n",
        "        for i in range(3):\n",
        "            print(\"Prediction\", i + 1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \", Score: \", topk.cpu().numpy()[0][i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predicting the image using the best tested model"
      ],
      "metadata": {
        "id": "rbgye-2CP_6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdskSFKwMlHL"
      },
      "outputs": [],
      "source": [
        "# Test a particular model on a test image\n",
        "'''\n",
        "! wget https://cdn.pixabay.com/photo/2017/01/20/00/30/maldives-1993704_960_720.jpg -O image.jpg\n",
        "\n",
        "model = torch.load(\"best_fine_tuned_model.pth\")\n",
        "predict(model, 'image.jpg')'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}